# 🧩 CLIP-Based Streamlit App

This project showcases a Streamlit app that utilizes the CLIP (Contrastive Language-Image Pre-Training) model to provide interactive image-text matching capabilities. CLIP is a powerful neural network trained on various (image, text) pairs, allowing it to predict the most relevant text snippet given an image.

---

## 🌟 Key Features

- 🖼️ **Image-Text Matching:** Predicts the most relevant text snippet for a given image.
- 📜 **Zero-Shot Learning:** Utilizes zero-shot capabilities similar to GPT-2 and GPT-3.
- 🖥️ **Interactive Streamlit App:** Provides a user-friendly interface for experimenting with CLIP.

---

## 🛠️ Technologies Used

- **Python:** Programming language for implementation.
- **Streamlit:** Framework for creating interactive web applications.
- **PyTorch:** Deep learning library used for the CLIP model.

---

## 🎥 Demo Video

Watch the demo video to see the CLIP-based Streamlit app in action:

[![CLIP-Based Streamlit App Demo](https://img.youtube.com/vi/mf6e5rpHGeM/0.jpg)](https://youtu.be/mf6e5rpHGeM)

---

## 🚀 Getting Started

### Prerequisites

- **Python 3.7+**
- **PyTorch 1.7.1+**
- **Streamlit**
- **torchvision**

### Installation

1. Clone the repository:
   ```sh
   git clone https://github.com/ahmadafzal007/Contrastive-Language-Image-Pre-Training.git
